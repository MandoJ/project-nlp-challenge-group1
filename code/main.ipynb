{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Group 1 — NLP Fake News \n",
        "\n",
        "**Project:** Fake News vs Real News (Tab-separated dataset)  \n",
        "**Bootcamp:** Ironhack — NLP Challenge  \n",
        "**Last updated:** 2026-02-04\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goal\n",
        "Build an NLP classifier that predicts whether a news item is **FAKE** or **REAL** (binary classification).\n",
        "\n",
        "### Deliverables (typical)\n",
        "- Clear preprocessing + feature engineering approach\n",
        "- One or more models with evaluation on a validation split\n",
        "- Final predictions on the **test** file\n",
        "- (Optional) Explainability: most informative features / error analysis\n",
        "- Reproducible code via **pipelines** and fixed random seeds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup & Imports\n",
        "- Keep imports centralized.\n",
        "- Use pipelines so preprocessing + vectorization + model training is one reproducible object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Viz (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "# Models (baselines)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Config\n",
        "Update these values to match your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======== CONFIG (EDIT IF NEEDED) ========\n",
        "DATA_DIR = Path(\"..\") / \"data\"          # notebook lives in /code, so .. points to repo root\n",
        "TRAIN_FILE = DATA_DIR / \"train.tsv\"\n",
        "TEST_FILE  = DATA_DIR / \"test.tsv\"\n",
        "\n",
        "# Column names (edit these after you inspect df.columns)\n",
        "TEXT_COL   = \"text\"     # e.g. \"text\", \"content\", \"article\", \"title_text\", etc.\n",
        "LABEL_COL  = \"label\"    # e.g. \"label\", \"target\", \"class\"\n",
        "\n",
        "# If your labels are strings like \"FAKE\"/\"REAL\", keep as-is.\n",
        "# If your labels are 0/1, also fine.\n",
        "# =========================================\n",
        "\n",
        "assert TRAIN_FILE.exists(), f\"Missing: {TRAIN_FILE}\"\n",
        "assert TEST_FILE.exists(), f\"Missing: {TEST_FILE}\"\n",
        "print(\" Files found:\", TRAIN_FILE.name, \"and\", TEST_FILE.name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load data\n",
        "Assumes **tab-separated** files. If you have a header row, this will infer it automatically.\n",
        "If you don't have headers, pass `header=None` and set column names.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load TSV\n",
        "train_df = pd.read_csv(TRAIN_FILE, sep=\"\\t\")\n",
        "test_df  = pd.read_csv(TEST_FILE, sep=\"\\t\")\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape :\", test_df.shape)\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Quick schema checks\n",
        "- Inspect columns\n",
        "- Check missing values\n",
        "- Confirm label distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Train columns:\", list(train_df.columns))\n",
        "print(\"Test columns :\", list(test_df.columns))\n",
        "\n",
        "# Basic NA checks\n",
        "display(train_df.isna().mean().sort_values(ascending=False).head(10))\n",
        "display(test_df.isna().mean().sort_values(ascending=False).head(10))\n",
        "\n",
        "# Label distribution (if LABEL_COL exists)\n",
        "if LABEL_COL in train_df.columns:\n",
        "    display(train_df[LABEL_COL].value_counts(dropna=False))\n",
        "else:\n",
        "    print(f\"⚠️ LABEL_COL='{LABEL_COL}' not found. Update CONFIG.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Define features and target\n",
        "We keep a clean split between:\n",
        "- `X` (text)\n",
        "- `y` (labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert TEXT_COL in train_df.columns, f\"TEXT_COL='{TEXT_COL}' not found in train_df.columns\"\n",
        "assert LABEL_COL in train_df.columns, f\"LABEL_COL='{LABEL_COL}' not found in train_df.columns\"\n",
        "assert TEXT_COL in test_df.columns,  f\"TEXT_COL='{TEXT_COL}' not found in test_df.columns\"\n",
        "\n",
        "X = train_df[TEXT_COL].astype(str)\n",
        "y = train_df[LABEL_COL]\n",
        "\n",
        "X_test = test_df[TEXT_COL].astype(str)\n",
        "\n",
        "print(\"X shape:\", X.shape, \"| y shape:\", y.shape, \"| X_test shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Baseline split (Train/Validation)\n",
        "We start with a **single stratified split** for quick iteration. Later you can add CV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Text cleaning (lightweight)\n",
        "This is intentionally conservative:\n",
        "- remove non-letters\n",
        "- collapse whitespace\n",
        "- lowercase\n",
        "\n",
        "> Important: With TF-IDF, you often get strong performance with *minimal* cleaning.\n",
        "If you add heavy cleaning (stemming/lemmatization), track whether it actually improves metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    text = str(text)\n",
        "    # Keep letters and spaces\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
        "    # Remove standalone single characters\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
        "    # Collapse spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Quick sanity check\n",
        "sample = X_train.iloc[0]\n",
        "print(\"BEFORE:\", sample[:200])\n",
        "print(\"AFTER :\", clean_text(sample)[:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Modeling approach\n",
        "We'll use **pipelines** so that:\n",
        "- text cleaning\n",
        "- vectorization (Count/TF-IDF)\n",
        "- classifier\n",
        "\n",
        "…are all packaged into a single object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "cleaner = FunctionTransformer(lambda s: pd.Series(s).apply(clean_text), validate=False)\n",
        "\n",
        "def make_pipeline(vectorizer, model):\n",
        "    return Pipeline(steps=[\n",
        "        (\"clean\", cleaner),\n",
        "        (\"vec\", vectorizer),\n",
        "        (\"clf\", model),\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Baseline models\n",
        "Try 2–3 baselines quickly and compare using the same metrics.\n",
        "Recommended quick baselines:\n",
        "- **TF-IDF + Logistic Regression**\n",
        "- **TF-IDF + LinearSVC**\n",
        "- **CountVectorizer + MultinomialNB**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"tfidf_logreg\": make_pipeline(\n",
        "        TfidfVectorizer(\n",
        "            ngram_range=(1,2),\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        ),\n",
        "        LogisticRegression(max_iter=2000, n_jobs=None)\n",
        "    ),\n",
        "    \"tfidf_linearsvc\": make_pipeline(\n",
        "        TfidfVectorizer(\n",
        "            ngram_range=(1,2),\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        ),\n",
        "        LinearSVC()\n",
        "    ),\n",
        "    \"count_mnb\": make_pipeline(\n",
        "        CountVectorizer(\n",
        "            ngram_range=(1,2),\n",
        "            min_df=2,\n",
        "            max_df=0.95\n",
        "        ),\n",
        "        MultinomialNB()\n",
        "    ),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, X_tr, y_tr, X_va, y_va, name=\"model\"):\n",
        "    t0 = time.perf_counter()\n",
        "    model.fit(X_tr, y_tr)\n",
        "    fit_s = time.perf_counter() - t0\n",
        "\n",
        "    y_pred = model.predict(X_va)\n",
        "\n",
        "    metrics = {\n",
        "        \"model\": name,\n",
        "        \"fit_seconds\": round(fit_s, 3),\n",
        "        \"accuracy\": accuracy_score(y_va, y_pred),\n",
        "        \"f1\": f1_score(y_va, y_pred, average=\"weighted\"),\n",
        "        \"precision\": precision_score(y_va, y_pred, average=\"weighted\", zero_division=0),\n",
        "        \"recall\": recall_score(y_va, y_pred, average=\"weighted\", zero_division=0),\n",
        "    }\n",
        "    return metrics, y_pred\n",
        "\n",
        "results = []\n",
        "preds_by_model = {}\n",
        "\n",
        "for name, pipe in models.items():\n",
        "    m, y_pred = evaluate(pipe, X_train, y_train, X_val, y_val, name=name)\n",
        "    results.append(m)\n",
        "    preds_by_model[name] = y_pred\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"f1\", ascending=False)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Best model deep-dive\n",
        "- Classification report\n",
        "- Confusion matrix\n",
        "- Error analysis (optional but valuable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_name = results_df.iloc[0][\"model\"]\n",
        "best_model = models[best_name]\n",
        "\n",
        "print(\"Best model:\", best_name)\n",
        "best_model.fit(X_train, y_train)\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "cm = confusion_matrix(y_val, y_val_pred, labels=np.unique(y))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\n",
        "disp.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Feature inspection (optional)\n",
        "For linear models, you can inspect top tokens for each class (if labels are binary).\n",
        "This helps you explain what the model is “using” to decide.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_features_linear(pipeline: Pipeline, top_n=20):\n",
        "    vec = pipeline.named_steps[\"vec\"]\n",
        "    clf = pipeline.named_steps[\"clf\"]\n",
        "\n",
        "    if not hasattr(clf, \"coef_\"):\n",
        "        print(\"This classifier doesn't expose coef_. Try LogisticRegression or LinearSVC.\")\n",
        "        return\n",
        "\n",
        "    feature_names = np.array(vec.get_feature_names_out())\n",
        "    coef = clf.coef_\n",
        "\n",
        "    # Binary case: coef shape (1, n_features)\n",
        "    if coef.shape[0] == 1:\n",
        "        weights = coef[0]\n",
        "        top_pos = feature_names[np.argsort(weights)[-top_n:]][::-1]\n",
        "        top_neg = feature_names[np.argsort(weights)[:top_n]]\n",
        "        print(\"\\nTop features (positive class):\")\n",
        "        print(top_pos)\n",
        "        print(\"\\nTop features (negative class):\")\n",
        "        print(top_neg)\n",
        "    else:\n",
        "        # Multiclass: show per class\n",
        "        for i, cls in enumerate(clf.classes_):\n",
        "            weights = coef[i]\n",
        "            top = feature_names[np.argsort(weights)[-top_n:]][::-1]\n",
        "            print(f\"\\nTop features for class={cls}:\")\n",
        "            print(top)\n",
        "\n",
        "# Fit then inspect\n",
        "best_model.fit(X_train, y_train)\n",
        "top_features_linear(best_model, top_n=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Cross-validation (recommended)\n",
        "Once you have a “best” pipeline, do stratified CV for a more reliable estimate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "cv_scores = cross_val_score(best_model, X, y, cv=skf, scoring=\"f1_weighted\")\n",
        "print(\"CV F1 (weighted):\", cv_scores.round(4))\n",
        "print(\"Mean:\", cv_scores.mean().round(4), \"| Std:\", cv_scores.std().round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Train final model on full training data\n",
        "Then generate predictions for the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model = best_model\n",
        "final_model.fit(X, y)\n",
        "\n",
        "test_pred = final_model.predict(X_test)\n",
        "pd.Series(test_pred).value_counts().head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Create submission file\n",
        "This is a generic template. Edit `ID_COL` if your test file includes an ID column.\n",
        "If not, we create one from the row index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======= OPTIONAL: ID COLUMN (EDIT IF NEEDED) =======\n",
        "ID_COL = \"id\"  # set to None if there's no ID column in test_df\n",
        "# ================================================\n",
        "\n",
        "if ID_COL is not None and ID_COL in test_df.columns:\n",
        "    submission = pd.DataFrame({ID_COL: test_df[ID_COL], LABEL_COL: test_pred})\n",
        "else:\n",
        "    submission = pd.DataFrame({\"id\": np.arange(len(test_df)), LABEL_COL: test_pred})\n",
        "\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_DIR = Path(\".\") / \"outputs\"\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "out_path = OUT_DIR / \"submission.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(\"✅ Saved:\", out_path.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) Notes for the presentation\n",
        "Use this section as a scratchpad for your group presentation:\n",
        "- What dataset columns exist?\n",
        "- What preprocessing did you choose and why?\n",
        "- What baselines did you test?\n",
        "- What is your best model and what metrics did it achieve?\n",
        "- What are common failure cases (misclassifications)?\n",
        "- Any improvements tried (stopword removal, n-grams, class weights, etc.)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your bullet points / TODOs here for the group.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
